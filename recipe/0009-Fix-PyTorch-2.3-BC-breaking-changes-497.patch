From d1e94fcfe7728240ae41d5febb0c314e03096814 Mon Sep 17 00:00:00 2001
From: Can Balioglu <cbalioglu@users.noreply.github.com>
Date: Fri, 3 May 2024 14:54:39 -0400
Subject: [PATCH 09/10] Fix PyTorch 2.3 BC breaking changes (#497)

---
 src/fairseq2/optim/dynamic_loss_scaler.py | 54 ++++++++++++++++-------
 1 file changed, 39 insertions(+), 15 deletions(-)

diff --git a/src/fairseq2/optim/dynamic_loss_scaler.py b/src/fairseq2/optim/dynamic_loss_scaler.py
index 50ec623b..12d1546c 100644
--- a/src/fairseq2/optim/dynamic_loss_scaler.py
+++ b/src/fairseq2/optim/dynamic_loss_scaler.py
@@ -8,7 +8,7 @@ import logging
 import math
 from dataclasses import dataclass
 from logging import Logger
-from typing import Any, Callable, Dict, Mapping, Optional, Tuple, cast
+from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Union, cast, final
 
 from torch import Tensor
 from torch.cuda.amp.grad_scaler import GradScaler
@@ -22,16 +22,13 @@ class DynamicLossScaler:
     """Performs loss scaling during backward pass to prevent underflow of half
     precision gradients."""
 
-    optimizer: Optimizer
-    gang: Gang
-    init_scale: float
-    scale_factor: float
-    scale_window: int
-    min_scale: float
-    logger: Optional[Logger]
-    enabled: bool
+    _optimizer: Optimizer
+    _scale_window: int
+    _min_scale: float
+    _is_enabled: bool
 
-    _grad_scaler: GradScaler
+    # TODO: consolidate into `GradScaler` once we cease support for PT2.2
+    _grad_scaler: Union[GradScaler, ShardedGradScaler]
 
     def __init__(
         self,
@@ -65,16 +62,43 @@ class DynamicLossScaler:
         :param enabled:
             If ``False``, disables loss scaling.
         """
-        if gang.size == 1:
-            self._grad_scaler = GradScaler(
-                init_scale, scale_factor, 1 / scale_factor, scale_window, enabled
+        if enabled:
+            for group in optimizer.param_groups:
+                for param in group["params"]:
+                    if param.dtype != torch.float32 and param.dtype != torch.float16:
+                        raise ValueError(
+                            f"The parameters held by `optimizer` must be of type `torch.float32` or `torch.float16`, but at least one parameter is of type `{param.dtype}` instead."
+                        )
+
+                    if param.device.type != "cuda":
+                        raise ValueError(
+                            f"The parameters held by `optimizer` must be on a `cuda` device, but at least one parameter is on a `{param.device.type}` device instead."
+                        )
+
+        if scale_window is None:
+            # The same formula that we use in fairseq.
+            scale_window = max(int(2**14 / gang.size / gradient_accumulation), 1)
+
+            log.info("The scale window is set to {}.", scale_window)
+
+        if not enabled or not sharded or gang.size == 1:
+            self._grad_scaler = _InternalGradScaler(
+                init_scale=init_scale,
+                growth_factor=scale_factor,
+                backoff_factor=1 / scale_factor,
+                growth_interval=scale_window,
+                enabled=enabled,
             )
         else:
             pg = gang.as_process_group()
 
-            # Yes, `growth_factor` and `backoff_factor` parameters are swapped.
             self._grad_scaler = ShardedGradScaler(
-                init_scale, 1 / scale_factor, scale_factor, scale_window, enabled, pg
+                init_scale=init_scale,
+                growth_factor=scale_factor,
+                backoff_factor=1 / scale_factor,
+                growth_interval=scale_window,
+                enabled=enabled,
+                process_group=pg,
             )
 
         self.optimizer = optimizer
-- 
2.39.5 (Apple Git-154)

